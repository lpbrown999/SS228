\section{Results}
The agent was initialized with no prior against a level 1 CPU and was allowed to train overnight, training the weights between each match. For training, we used a learning rate of $\alpha = 0.01$, a discount factor of $\gamma = 0.95$, and a non-fixed soft-max exploration parameter $\lambda$. The agent was trained against progressively harder opponents while using priors from the previous training, until we reached a level 4 CPU.

In Figure~\ref{winpctg}, we see that the uninformed agent with a high exploration bias quickly achieves an approximately 80 percent win-rate against the base level 1 opponent. When this agent is then given a lower exploration bias, that win-rate quickly converges to 100\%. The first real challenge faced is against the level 3 opponent, were the agent undergoes a difficult period, having its win-rate fall to near 50\% before recovering back to 100\% over a period of approximately 100 games. A similar trend is observed when the agent faces off with a level 4 opponent. We can also see similar trends in Figures~\ref{stocks}, and~\ref{damage} where the stock differential and the damage differential slowly improve as the agent trains. 

In addition to the quantitative performance metrics observed in the Figures, the agents behavior also improved qualitatively. Against the level 1 opponent, the agent learned basic behaviors that lead to a win. The agent would simply repeat the same attacking move that the level 1 opponent was unable to deal with. As the agent faced progressively more difficult opponents (level 3 and 4), this behavior would sometimes work but would often result in the agent taking damage. The agent was able to determine states in which this learned "spamming" behavior was optimal and which states it should avoid taking this action in, developing new strategies depending on the state.

An interesting trend in the data is the increase in games required for the agent to learn an effective policy against its opponents. Against the level 1 CPU, we see that the winning strategy can be learned in approximately 85 games. Against the level 3, this process takes 85 games, and against the level 4 CPU we were only able to achieve an 85\% winrate after 245 games. 



%Beginning from no prior knowledge, the agent quickly progressed to wining games against its opponent, preserving its own stocks, as well as increasing its relative damage output. This trend carries over into the other sessions as well and can be observed in figures ONE TO THREE. An interesting note is that the performance of the level 1 CPU trained agent against the level 3 CPU initially declines from its initialized behavior before recovering to a 100 percent win-rate. 

