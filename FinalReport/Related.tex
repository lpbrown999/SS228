\section{Related Work}

Q-learning is a popular reinforcement learning strategy however the state space and action spaces are often continuous and cannot be represented in tabular form. In~\cite{gaskett}, Gasket et al implemented a novel interpolator to approximate the Q-function with state and action generalization. In addition to a continuous state space, applying reinforcement learning in an adversarial environment can prove to be difficult because the environment is working against the agent. In~\cite{uther}, a generalized reinforcement learning algorithm is applied to an agent in an adversarial environment. Both~\cite{gaskett} and~\cite{uther} show that applying generalization to reinforcement learning is viable solution to dealing with continuous state-action spaces and adversarial environments. In a recent advancement in generalization for reinforcement learning, Emigh et al applied a nearest neighbor local approximation reinforcement learning algorithm in~\cite{emigh} to Frogger, allowing generalization of the state space based on state proximity. This proves to be an effective strategy in environments with limited data and large similarity between optimality in nearby states.

Another difficulty in reinforcement learning is assigning credit to the action the led to the reward. Take for example winning the lottery, there is a large delay between when we but the lottery ticket and when we get the reward. If we are drinking coffee right before we find out we win the lottery, the reward of winning the lottery should still be assigned to buying the ticket and not to drinking coffee. Sutton et al developed a method in~\cite{sutton} to properly assign credit to the action or sequence of actions that lead to a reward. This problem can become increasingly difficult to address as the dimensionality of the problem increases.       
