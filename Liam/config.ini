[DEFAULT]
#check betalib, rewardlib
beta_function = 5
reward_function = 2

thetaFolderRoot = thetas
logFolderRoot = logs

#Styles: none, jumper or random
#Actors: AI or Human
[Agent1]
actor = AI
style = random

thetaFolder = jumper_11_14
thetaFile = theta_beta5_reward2_noprior.npy
#test is for using no ay, 0 instead in beta 5

logFile = Agent1LogRandOvernight.csv
logFolder = Agent1Logs

[Agent2]
actor = AI
style = empty

thetaFolder = jumper_11_14
thetaFile = theta_beta5_reward2_noprior_test.npy

logFile = Agent2Log.csv
logFolder = Agent2Logs

[BatchLearn]
#Prior is either none or a thing
thetaFolder = jumper_11_14

thetaPrior  = none
thetaOutput = theta_beta5_reward2_noprior_test.npy

logFile = PriorWeights3hrs.csv
logFolder = Agent1Logs

alpha = .009
gamma = .00001
numActions = 54
iterations = 10
