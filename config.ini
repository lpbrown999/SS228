[DEFAULT]
#check betalib, rewardlib
beta_function = 5
reward_function = 2

thetaFolderRoot = thetas
logFolderRoot = logs

#Styles: none, jumper or random
#Actors: AI or Human



[Agent1]
actor = Human
style = random

thetaFolder = jumper_11_16
thetaFile = th_b5_jump_prior.npy

logFile = Agent1Log.csv
logFolder = Agent1Logs




[Agent2]
actor = AI
style = jumper

thetaFolder = jumper_11_16
thetaFile = th_b5_r2_overnight_noprior_withgamma.npy

logFile = Agent2Log.csv
logFolder = Agent2Logs


[BatchLearn]
#Prior is either none or a thing

thetaPrior  = none
thetaOutput = th_b5_r2_overnight_noprior_withgamma.npy

logFile = Agent1LogRandOvernight.csv
logFolder = Agent1Logs
thetaFolder = jumper_11_16

alpha = .01
gamma = .95
numActions = 54
iterations = 1
