\section{Conclusion}

Applying perceptron Q-learning for Super Smash Brothers Melee was successful. Given enough time, the agent learned to progressively beat higher level default CPUs, die less, and take less damage. One severely limiting factor of the project was the need to run the agent in real time against a CPU in order to train. This caused iteration on the basis functions, reward functions, hyper parameters as well as progression through CPU difficulty to be slow. On the other hand, the combination of our achieved performance and low iterations on parameters and functions shows the robustness of perceptron Q-learning in this environment. 

We believe that with improvement to the reward functions and careful detail to to hyper parameters, as well as more time, the agent could learn to beat significantly higher level CPUs than demonstrated here. Additionally, while the agent has only ever played as captain falcon against captain falcon, the agent should be able to learn to play any matchup and on any stage. This could be done by storing different basis function weights for each situation.

Our group plans to continue the project by both continuing to play against higher level agents with the existing perceptron Q-learning approach, as well as extending the bots capabilities by implementing DQN. We believe DQN will provide a higher level of performance as it is able to represent non-linear basis functions, while perceptron Q-learning is limited to linear relationships. We are also looking to explore different characters and stages.

