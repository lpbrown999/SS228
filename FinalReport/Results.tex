\section{Results}
The agent was initialized with no prior against a level 1 CPU and was allowed to train overnight, training the weights between each match. For training, we used a learning rate of $\alpha = 0.01$, a discount factor of $\gamma = 0.95$, and a non-fixed soft-max exploration parameter $\lambda$. The agent was trained against progressively harder opponents while using priors from the previous training, until we reached a level 4 CPU. 

We can see in Figure~\ref{winpctg} that the win percentage converges to 100\% for level 1 opponents and the win percentage for level 3 and level 4 opponents increases to around 80\%. We can also see similar trends in Figures~\ref{stocks}, and~\ref{damage} where the stock differential and the damage differential progressively get better with more training.


%Beginning from no prior knowledge, the agent quickly progressed to wining games against its opponent, preserving its own stocks, as well as increasing its relative damage output. This trend carries over into the other sessions as well and can be observed in figures ONE TO THREE. An interesting note is that the performance of the level 1 CPU trained agent against the level 3 CPU initially declines from its initialized behavior before recovering to a 100 percent win-rate. 

