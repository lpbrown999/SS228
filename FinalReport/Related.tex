\section{Related Work}

Q-learning is a popular reinforcement learning strategy due to its ability to learn complex tasks without little to no knowledge of the underlying dynamics. Often, the state space and action spaces are continuous and cannot be represented in tabular form. In~\cite{gaskett}, Gasket et al implemented a novel interpolator to approximate the Q-function with state and action generalization a continuous space. In addition to the difficulty of learning from a continuous state space, applying reinforcement learning in an adversarial environment can prove to be difficult because the environment is working against the agent. In~\cite{uther}, a generalized reinforcement learning algorithm is applied to anagent in an adversarial environment. Both~\cite{gaskett} and~\cite{uther} show that applying generalization to reinforcement learning is viable solution to dealing with the complexity of discretizing state and action spaces. 

In a recent advancement in generalization for reinforcement learning, Emigh et al applied a nearest neighbor local approximation reinforcement learning algorithm in~\cite{emigh} to Frogger, allowing generalization of the state space based on state proximity. This proves to be an effective strategy in environments with limited data and large similarity between optimality in nearby states.

Another difficulty in reinforcement learning is assigning credit to the action the led to the reward. Depending on the dynamics of the environment that reinforcement learning is being applied to, the reward from executing an action can be delayed. Take for example winning the lottery, there is a large delay between when we but the lottery ticket and when we get the reward. If we are drinking coffee right before we find out we win the lottery, the reward of winning the lottery should still be assigned to buying the ticket and not to drinking coffee. Sutton et al developed a method in~\cite{sutton} to properly assign credit to the action or sequence of actions that lead to a reward. This problem can become increasingly difficult to address as the dimensionality of the problem increases.       
